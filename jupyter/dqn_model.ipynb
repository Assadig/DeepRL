{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.engine.topology import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.layers import Dropout, Reshape\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from memory.ipynb\n"
     ]
    }
   ],
   "source": [
    "# local library\n",
    "from memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    \"\"\"Deep Q-Learning Networ\n",
    "    \n",
    "    Basend on DQN and Multiscale CNN, find the optimal time to \n",
    "    exit from a stock market.\n",
    "    \n",
    "    Available function\n",
    "    - build_model: build network based on tensorflow and keras\n",
    "    - train: given DateFrame stock data, train network\n",
    "    - predict_action: givne DataFrame stock data, return optimal protfolio\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"initialized approximate value function\n",
    "        \n",
    "        config should have the following attributes\n",
    "        \n",
    "        Args:\n",
    "            device: the device to use computation, e.g. '/gpu:0'\n",
    "            gamma(float): the decay rate for value at RL\n",
    "            history_length(int): input_length for each scale at CNN\n",
    "            n_feature(int): the number of type of input \n",
    "                (e.g. the number of company to use at stock trading)\n",
    "            n_history(int): the nubmer of history that will be used as input\n",
    "            n_smooth, n_down(int): the number of smoothed and down sampling input at CNN\n",
    "            k_w(int): the size of filter at CNN\n",
    "            n_hidden(int): the size of fully connected layer\n",
    "            n_batch(int): the size of mini batch\n",
    "            n_epochs(int): the training epoch for each time\n",
    "            update_rate (0, 1): parameter for soft update\n",
    "            learning_rate(float): learning rate for SGD\n",
    "            memory_length(int): the length of Replay Memory\n",
    "            n_memory(int): the number of different Replay Memories\n",
    "            alpha, beta: [0, 1] parameters for Prioritized Replay Memories\n",
    "        \"\"\"\n",
    "        self.device = config.device\n",
    "        self.save_path = config.save_path\n",
    "        self.is_load = config.is_load\n",
    "        self.gamma = config.gamma\n",
    "        self.history_length = config.history_length\n",
    "        self.n_task = config.n_task\n",
    "        self.n_feature = config.n_feature\n",
    "        self.n_smooth = config.n_smooth\n",
    "        self.n_down = config.n_down\n",
    "        self.k_w = config.k_w\n",
    "        self.n_hidden = config.n_hidden\n",
    "        self.n_batch = config.n_batch\n",
    "        self.n_epochs = config.n_epochs\n",
    "        self.update_rate = config.update_rate\n",
    "        self.alpha = config.alpha\n",
    "        self.beta = config.beta\n",
    "        self.lr = config.learning_rate\n",
    "        self.memory_length = config.memory_length\n",
    "        self.n_memory = config.n_memory\n",
    "        # the length of the data as input\n",
    "        self.n_history = max(self.n_smooth + self.history_length, (self.n_down + 1) * self.history_length)\n",
    "        print (\"building model....\")\n",
    "        # have compatibility with new tensorflow\n",
    "        tf.python.control_flow_ops = tf\n",
    "        # avoid creating _LEARNING_PHASE outside the network\n",
    "        K.clear_session()\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "        K.set_session(self.sess)\n",
    "        with self.sess.as_default():\n",
    "            with tf.device(self.device):\n",
    "                self.build_model()\n",
    "        print('finished building model!')\n",
    "    \n",
    "    def train(self, input_data, noise_scale=0.1):\n",
    "        \"\"\"training DQN, which has two actions: 0-exit, 1-stay\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame): stock price for self.n_feature companies\n",
    "        \"\"\"\n",
    "        task_data = input_data.values\n",
    "        date = input_data.index\n",
    "        T = len(task_data)\n",
    "        self.noise_scale = noise_scale\n",
    "        \n",
    "        # frequency for output\n",
    "        print_freq = int(T / 100)\n",
    "        if print_freq == 0:\n",
    "            print_freq = 1\n",
    "        print (\"training....\")\n",
    "        st = time.time()\n",
    "        #  udpate rate for prioritizing parameter\n",
    "        db = (1 - self.beta) / 1000\n",
    "        \n",
    "        # result for return value\n",
    "        values = [[] for _ in range(self.n_task)]\n",
    "        date_label = [[] for _ in range(self.n_task)]\n",
    "        date_use = []\n",
    "        task_use = []\n",
    "        # will not train until getting enough data\n",
    "        t0 = self.n_history + self.n_batch\n",
    "        self.initialize_memory(task_data[:t0], scale=noise_scale)\n",
    "        save_data_freq = 10\n",
    "        save_weight_freq = 10\n",
    "        count = 0\n",
    "        input_data.to_csv(\"task_price.csv\")\n",
    "        for t in range(t0, T):\n",
    "            task_use.append(task_data[t])\n",
    "            date_use.append(date[t])\n",
    "            action = self.predict_action(task_data[t])\n",
    "            for i in range(self.n_task):\n",
    "                if action[i] == 0:\n",
    "                    date_label[i].append(date[t])\n",
    "                    values[i].append(task_data[t][i])\n",
    "            self.update_memory(task_data[t])\n",
    "            count += 1\n",
    "            for epoch in range(self.n_epochs):    \n",
    "                # select transition from pool\n",
    "                self.update_weight()\n",
    "                # update prioritizing paramter untill it goes over 1\n",
    "            self.beta  += db\n",
    "            if self.beta >= 1.0:\n",
    "                self.beta = 1.0\n",
    "            idx = np.random.randint(0, self.n_memory)\n",
    "            \n",
    "            experiences, weights = self.memory[idx].sample(self.n_batch, self.n_history, self.alpha, self.beta)\n",
    "            max_idx = self.get_max_idx(experiences.state1)\n",
    "            target_value = self.sess.run(self.target_value,\n",
    "                                     feed_dict={self.state_target: experiences.state1,\n",
    "                                 self.reward: experiences.reward,\n",
    "                                               self.max_idx_target: max_idx})\n",
    "            \n",
    "            if t % print_freq == 0:\n",
    "                print (\"time:\",  date[t])\n",
    "                error = self.sess.run(self.error,\n",
    "                              feed_dict={self.state: experiences.state0,\n",
    "                                         self.target: target_value,\n",
    "                                         self.reward: experiences.reward,\n",
    "                                         K.learning_phase(): 0})\n",
    "                print(\"error:\", np.mean(error))\n",
    "                action = self.predict_action(task_data[t])\n",
    "                print(\"portfolio:\", action)\n",
    "                print (\"elapsed time\", time.time() - st)\n",
    "                print(\"********************************************************************\")\n",
    "                \n",
    "            if count % save_data_freq == 0:\n",
    "                for i in range(self.n_task):\n",
    "                    result = pd.DataFrame(values[i], index=pd.DatetimeIndex(date_label[i]))\n",
    "                    result.to_csv(\"exit_result_{}.csv\".format(i))\n",
    "                data_use = pd.DataFrame(task_use, index=pd.DatetimeIndex(date_use))\n",
    "                data_use.to_csv(\"task_price.csv\")\n",
    "                \n",
    "            if count % save_weight_freq == 0:\n",
    "                save_path = self.saver.save(self.sess, self.save_path)\n",
    "                print(\"Model saved in file: %s\" % self.save_path)\n",
    "\n",
    "        save_path = self.saver.save(self.sess, self.save_path)\n",
    "        print(\"Model saved in file: %s\" % self.save_path)\n",
    "        print (\"finished training\")\n",
    "        \n",
    "        return [pd.DataFrame(values[i], index=pd.DatetimeIndex(date_label[i])) for i in range(self.n_task)]\n",
    "    \n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Preduct Optimal strategy\n",
    "        \n",
    "        Args:\n",
    "            state(float): stock data with size: [self.n_task, ]\n",
    "        Retrun:\n",
    "            integer: 0-exit, 1-stay\n",
    "        \"\"\"\n",
    "        pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n",
    "        new_state = pred_state[-1]\n",
    "        new_state = np.concatenate((new_state[1:], [state]), axis=0)\n",
    "        pred_state = np.concatenate((pred_state[:-1], [new_state]), axis=0)\n",
    "        action = self.max_action.eval(\n",
    "            session=self.sess,\n",
    "            feed_dict={self.state: pred_state, K.learning_phase(): 0})[-1]\n",
    "        return action\n",
    "    \n",
    "    def update_weight(self):\n",
    "        \"\"\"Update networks' parameters and memories\"\"\"\n",
    "        idx = np.random.randint(0, self.n_memory)\n",
    "        experiences, weights = self.memory[idx].sample(self.n_batch, self.n_history, self.alpha, self.beta)\n",
    "        max_idx = self.get_max_idx(experiences.state1)\n",
    "        # get target value for optimization\n",
    "        target_value = self.sess.run(self.target_value,\n",
    "                                     feed_dict={self.state_target: experiences.state1,\n",
    "                                 self.reward: experiences.reward,\n",
    "                                               self.max_idx_target: max_idx})\n",
    "        # optimize network\n",
    "        self.sess.run(self.critic_optim, \n",
    "                      feed_dict={self.state: experiences.state0,\n",
    "                                 self.target: target_value,\n",
    "                                 self.weights: weights,\n",
    "                                 self.learning_rate: self.lr,\n",
    "                                 K.learning_phase(): 1})  \n",
    "        # compute errors to determine prioritizing ratio\n",
    "        error = self.sess.run(self.error,\n",
    "                              feed_dict={self.state: experiences.state0,\n",
    "                                         self.target: target_value,\n",
    "                                         self.reward: experiences.reward,\n",
    "                                         K.learning_phase(): 0})\n",
    "        self.memory[idx].update_priority(error)\n",
    "        # softupdate for critic network\n",
    "        old_weights = self.critic_target.get_weights()\n",
    "        new_weights = self.critic.get_weights()\n",
    "        weights = [self.update_rate * new_w + (1 - self.update_rate) * old_w\n",
    "                   for new_w, old_w in zip(new_weights, old_weights)]\n",
    "        self.critic_target.set_weights(weights)\n",
    "        \n",
    "    def initialize_memory(self, tasks, scale=10):\n",
    "        self.memory = []\n",
    "        for i in range(self.n_memory):\n",
    "            self.memory.append(SequentialMemory(self.memory_length))\n",
    "        for t in range(len(tasks)):\n",
    "            for idx_memory in range(self.n_memory):\n",
    "                action = None\n",
    "                reward = np.concatenate((np.reshape(tasks[t], (self.n_task, 1)), np.zeros((self.n_task, 1))), axis=-1)\n",
    "                self.memory[idx_memory].append(tasks[t], action, reward)\n",
    "        \n",
    "    def update_memory(self, state):\n",
    "        \"\"\"Update memory without updating weight\"\"\"\n",
    "        for i in range(self.n_memory):\n",
    "            self.memory[i].observations.append(state)\n",
    "            self.memory[i].priority.append(1.0)\n",
    "        # to stabilize batch normalization, use other samples for prediction\n",
    "        pred_state = self.memory[0].sample_state_uniform(self.n_batch, self.n_history)\n",
    "        for i in range(self.n_memory):\n",
    "            action_off = None\n",
    "            reward_off = np.concatenate((np.reshape(state, (self.n_task, 1)), np.zeros((self.n_task, 1))), axis=-1)\n",
    "            self.memory[i].rewards.append(reward_off)\n",
    "            self.memory[i].actions.append(action_off)\n",
    "    \n",
    "    def get_max_idx(self, state):\n",
    "        max_action = self.sess.run(self.max_action_target, feed_dict={self.state_target: state})\n",
    "        shape = max_action.shape\n",
    "        max_idx = []\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                max_idx.append([i, j, max_action[i][j]])\n",
    "        return np.array(max_idx, dtype=int)\n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        # transform input into the several scales and smoothing\n",
    "        self.state =  tf.placeholder(tf.float32, [None, self.n_history, self.n_task], name='state')\n",
    "        self.state_target = tf.placeholder(tf.float32, [None, self.n_history, self.n_task], name='state_target')\n",
    "        # reshape to convolutional input\n",
    "        state_ = tf.reshape(self.state, [-1, self.n_history, self.n_task, 1])\n",
    "        state_target_ = tf.reshape(self.state_target, [-1, self.n_history, self.n_task, 1])\n",
    "        raw, smoothed, down = self.transform_input(state_)\n",
    "        raw_target, smoothed_target, down_target = self.transform_input(state_target_)\n",
    "        \n",
    "        # build graph for citic training\n",
    "        input_q = [raw,] +  smoothed + down\n",
    "        self.Q = self.critic(input_q)\n",
    "        self.max_action = tf.argmax(self.Q, dimension=2)\n",
    "        # target network\n",
    "        input_q_target = [raw_target,] +  smoothed_target + down_target\n",
    "        Q_target = self.critic_target(input_q_target)\n",
    "        self.reward = tf.placeholder(tf.float32, [None, self.n_task, 2], name='reward')\n",
    "        double_Q = self.critic(input_q_target)\n",
    "        self.max_action_target = tf.argmax(double_Q, 2)\n",
    "        self.max_idx_target = tf.placeholder(tf.int32, [None, 3], \"double_idx\")\n",
    "        Q_max = tf.gather_nd(Q_target, self.max_idx_target)\n",
    "        Q_max = tf.reshape(Q_max, [-1, self.n_task, 1])\n",
    "        Q_value = tf.concat(2, (tf.zeros_like(Q_max), Q_max))\n",
    "        self.target_value = self.reward  + self.gamma * Q_value\n",
    "        self.target_value = tf.cast(self.target_value, tf.float32)\n",
    "        self.target = tf.placeholder(tf.float32, [None, self.n_task, 2], name=\"target_value\")\n",
    "        # optimization\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        # get rid of bias of prioritized\n",
    "        self.weights = tf.placeholder(tf.float32, shape=[None], name=\"weights\")\n",
    "        self.loss = tf.reduce_mean(self.weights * tf.reduce_sum(tf.square(self.target - self.Q), [1, 2]), name='loss')\n",
    "        # TD-error for priority\n",
    "        self.error = tf.reduce_sum(tf.abs(self.target - self.Q), [1, 2])\n",
    "        self.critic_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(self.loss, var_list=self.critic.trainable_weights)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        is_initialize = True\n",
    "        if self.is_load:\n",
    "            if self.load(self.save_path):\n",
    "                print('succeded to load')\n",
    "                is_initialize = False\n",
    "            else:\n",
    "                print('failed to load')\n",
    "        \n",
    "        # initialize network\n",
    "        tf.initialize_all_variables().run(session=self.sess)\n",
    "        weights = self.critic.get_weights()\n",
    "        self.critic_target.set_weights(weights)\n",
    "        \n",
    "    def build_critic(self):\n",
    "        \"\"\"Build critic network\n",
    "        \n",
    "        recieve transformed tensor: raw_data, smooted_data, and downsampled_data\n",
    "        \"\"\"\n",
    "        nf = self.n_feature\n",
    "        # layer1\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth)]\n",
    "        for m in sm_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_task, 1)))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down)]\n",
    "        for m in dw_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_task, 1)))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input\n",
    "        state = Sequential()\n",
    "        nf = self.n_feature\n",
    "        state.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_task, 1)))\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat', concat_axis=-1)\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        model.add(BatchNormalization(mode=2, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        model.add(Flatten())\n",
    "        # layer3\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # layer4\n",
    "        model.add(Dense(int(np.sqrt(self.n_hidden))))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        model.add(Dense(2 * self.n_task))\n",
    "        model.add(Reshape((self.n_task, 2)))\n",
    "        return model\n",
    "    \n",
    "    def transform_input(self, input):\n",
    "        \"\"\"Transform data into the Multi Scaled one\n",
    "        \n",
    "        Args:\n",
    "            input: tensor with shape: [None, self.n_history, self.n_task]\n",
    "        Return:\n",
    "            list of the same shape tensors, [None, self.length_history, self.n_task]\n",
    "        \"\"\"\n",
    "        # the last data is the newest information\n",
    "        raw = input[:, self.n_history - self.history_length:, :, :]\n",
    "        # smooth data\n",
    "        smoothed = []\n",
    "        for n_sm in range(2, self.n_smooth + 2):\n",
    "            smoothed.append(\n",
    "                tf.reduce_mean(tf.pack([input[:, self.n_history - st - self.history_length:self.n_history - st, :, :]\n",
    "                                        for st in range(n_sm)]),0))\n",
    "        # downsample data\n",
    "        down = []\n",
    "        for n_dw in range(2, self.n_down + 2):\n",
    "            sampled_ = tf.pack([input[:, idx, :, :] \n",
    "                                for idx in range(self.n_history-n_dw*self.history_length, self.n_history, n_dw)])\n",
    "            down.append(tf.transpose(sampled_, [1, 0, 2, 3]))\n",
    "        return raw, smoothed, down\n",
    "    \n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        try:\n",
    "            self.saver.restore(self.sess, self.save_path)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-pricing",
   "language": "python",
   "name": "drl-pricing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
